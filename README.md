## FABRIC Q&A Tool 
Welcome to FABRIC's Q&A tool repository! This repository contains the application files for FABRIC's production Q&A tool. Check out the tool here: https://portal.fabric-testbed.net

### Overview of Q&A tool
The Q&A tool is implemented using a Retreival Augemented Generation (RAG) architecture. Answers in the tool are generated by an LLM--and the LLM utilizes FABRIC context to generate its answer. Context is retrieved from FABRIC's official knowledge base and forums. 

### Project Structure

```
fabric-ai-tool-deployment/
├── app.py                  # Flask application entry point
├── config.py               # Loads environment variables & global config
├── .env                    # Secrets & environment configuration
├── requirements.txt        # Python dependencies
├── utils/                  # Utility modules
│   ├── logging_setup.py    # Centralized logging configuration
│   └── helpers/            # Helper functions
│       ├── assign_params.py            # Assign hyperparameters
│       ├── initialize_dependencies.py  # initialize models, retrievers etc
│       ├── print_helpers.py            # format output
│       ├── rerank_helpers.py           # help in reranking step
│       └── validate_api_call.py        # validate API call
└── README.md
```

### Example .env 

```
# Flask App Secret
FLASK_SECRET_KEY=<flask-secret-key>

# OpenAI Secret
OPEN_AI_SECRET=<openai-secret-key>

# Vector Databases for RAG
QA_DB_FILE=<location of vectorsotre for QA tool>
CG_DB_FILE=<location of vectorsotre for CG tool>

# LLMs 
QA_MODEL=<LLM for QA tool>
CG_MODEL=<LLM for CG tool>

# System Prompts
QA_PROMPT=<system prompt for QA tool>
CG_PROMPT=<system prompt for CG tool>

# Logging
LOG_DIR=<location of log directory>

# Deployment 
HOST=<host-url>
PORT=<port-number>
```

### Example system prompts

```
# For Q&A tool
"You are an AI Help Desk assistant for FABRIC. If you're asked questions that are not related to FABRIC or FABRIC questions that are very different from the context provided, simply say you cannot help.  Use the following information to answer the question below. {context} Question: On FABRIC Testbed, {question} Here is the answer based on the given information:"

# For Code Generation tool
"You are an AI Code Assstant. Use the following pieces of context (examples) to generate python code to implement the user's question specifically for FABRIC testbed. Use FablibManager whenever possible. Make sure to include correct import statements.Generate the answer in Markdown. If the question is very different from the context provided or if the question is not related to FABRIC, simply say you cannot help. {context} Question: On FABRIC Testbed, {question} Use FablibManager as much as possible. Include import statements. Here is how you will implement that (in markdown):"
```

### Important Notes
- The code makes use of the sentence-transformer module to embed and compare sentences. This requires a GPU, so ensure you're running the app on GPU
- Currently, the formatting on the responses assumes the model is gpt-4o-mini, so change the formatting accordingly when working other models 
