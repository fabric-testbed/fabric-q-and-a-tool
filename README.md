## About this repo

This is the code that is currently being used for deployment.

## FABRIC Q&A Tool 
Welcome to FABRIC's Q&A tool repository! This repository contains the application files for FABRIC's production Q&A tool. Check out the tool here: https://portal.fabric-testbed.net

### Overview of Q&A tool
The Q&A tool is implemented using a Retreival Augemented Generation (RAG) architecture. Answers in the tool are generated by an LLM--and the LLM utilizes FABRIC context to generate its answer. Context is retrieved from FABRIC's official knowledge base and forums. 

### Project Structure

```
fabric-ai-tool-deployment/
├── app.py                  # Flask application entry point
├── config.py               # Loads environment variables & global config
├── .env                    # Secrets & environment configuration
├── pyproject.toml          # Python dependencies and project config
├── utils/                  # Utility modules
│   ├── logging_setup.py    # Centralized logging configuration
│   └── helpers/            # Helper functions
│       ├── assign_params.py            # Assign hyperparameters
│       ├── initialize_dependencies.py  # initialize models, retrievers etc
│       ├── print_helpers.py            # format output
│       ├── rerank_helpers.py           # help in reranking step
│       └── validate_api_call.py        # validate API call
├── notebooks/              # Development notebooks for VectorDB creation
│   ├── README.md           # Detailed notebook documentation
│   ├── 01_data_preprocessing/      # Clean CSV/PDF data
│   ├── 02_vectordb_creation/       # Build ChromaDB databases
│   ├── 03_analysis/                # Evaluate retrieval quality
│   └── 04_testing/                 # Test RAG pipeline end-to-end
├── data/                   # Local data directory (gitignored)
│   ├── raw/                # Original CSV, PDF source files
│   ├── processed/          # Cleaned data ready for embedding
│   └── vectordbs/          # Generated ChromaDB databases
│       ├── qa_tool/        # Vector DB for Q&A tool
│       └── code_gen/       # Vector DB for Code Generation tool
└── README.md
```

### Example .env 

```
# Flask App Secret
FLASK_SECRET_KEY=<flask-secret-key>

# OpenAI Secret
OPEN_AI_SECRET=<openai-secret-key>

# Vector Databases for RAG
QA_DB_FILE=./data/vectordbs/qa_tool/
CG_DB_FILE=./data/vectordbs/code_gen/

# LLMs 
QA_MODEL=<LLM for QA tool>
CG_MODEL=<LLM for CG tool>

# System Prompts
QA_PROMPT=<system prompt for QA tool>
CG_PROMPT=<system prompt for CG tool>

# Logging
LOG_DIR=<location of log directory>

# Deployment
HOST=<host-url>
PORT=<port-number>

# Remote Ollama Server (optional)
OLLAMA_BASE_URL=https://<remote-server-host>:<port>
OLLAMA_VERIFY_SSL=false  # set to "false" only for self-signed certs; defaults to true
```

### Example system prompts

```
# For Q&A tool
"You are an AI Help Desk assistant for FABRIC. If you're asked questions that are not related to FABRIC or FABRIC questions that are very different from the context provided, simply say you cannot help.  Use the following information to answer the question below. {context} Question: On FABRIC Testbed, {question} Here is the answer based on the given information:"

# For Code Generation tool
"You are an AI Code Assstant. Use the following pieces of context (examples) to generate python code to implement the user's question specifically for FABRIC testbed. Use FablibManager whenever possible. Make sure to include correct import statements.Generate the answer in Markdown. If the question is very different from the context provided or if the question is not related to FABRIC, simply say you cannot help. {context} Question: On FABRIC Testbed, {question} Use FablibManager as much as possible. Include import statements. Here is how you will implement that (in markdown):"
```

### Important Notes
- The code makes use of the sentence-transformer module to embed and compare sentences. This requires a GPU, so ensure you're running the app on GPU
- Currently, the formatting on the responses assumes the model is gpt-4o-mini, so change the formatting accordingly when working other models
- **Remote Ollama Server**: By default, Ollama models connect to `http://localhost:11434`. To use a remote Ollama instance over HTTPS, set `OLLAMA_BASE_URL` in your `.env`. If the remote server uses a self-signed certificate, also set `OLLAMA_VERIFY_SSL=false`

## Creating/Updating Vector Databases

The vector databases used by the Q&A and Code Generation tools are created using Jupyter notebooks in the `notebooks/` directory.

### Quick Start
1. Place your raw data files (CSV, PDF) in `data/raw/`
2. Follow the numbered notebook workflow:
   - `01_data_preprocessing/` - Clean and prepare data
   - `02_vectordb_creation/` - Build ChromaDB databases
   - `03_analysis/` - Validate retrieval quality
   - `04_testing/` - Test RAG pipeline end-to-end
3. Update `.env` to point to the new databases:
   ```bash
   QA_DB_FILE=./data/vectordbs/qa_tool/
   CG_DB_FILE=./data/vectordbs/code_gen/
   ```

For detailed instructions, see [notebooks/README.md](notebooks/README.md). 
