{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13addae4-cb04-4494-9955-5ca2ec45cd65",
   "metadata": {},
   "source": [
    "# Analyze Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbaa76c-33ca-4e47-95a1-099e2b9ea339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa98434-1313-4b11-91ec-0ce941be6901",
   "metadata": {},
   "source": [
    "## Specify the embedding model and vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nrbw9ze234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "# Change this to analyze different databases\n",
    "DATABASE_OPTIONS = {\n",
    "    \"qa\": \"../../data/vectordbs/qa_tool/Cleaned_DB\",\n",
    "    \"code_gen\": \"../../data/vectordbs/code_gen/\",\n",
    "    \"custom\": \"my_vector_store\"  # For ad-hoc testing\n",
    "}\n",
    "\n",
    "ANALYZE_DB = \"qa\"  # <<< Change this to switch databases\n",
    "# ===================================\n",
    "\n",
    "database_loc = DATABASE_OPTIONS[ANALYZE_DB]\n",
    "print(f\"Analyzing database: {ANALYZE_DB}\")\n",
    "print(f\"Path: {database_loc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae17a76d-4284-4dc4-bf2f-2914f622055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "\n",
    "vectorstore = Chroma(persist_directory=database_loc,\n",
    "      embedding_function=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea5fad-15d3-42f2-9c8a-6813e58a1ae2",
   "metadata": {},
   "source": [
    "### (optional) Print the contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3046f1f2-683c-4274-84e8-4a5772df937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = vectorstore.get()['documents']\n",
    "\n",
    "print(f\"docs: {len(all_docs)}\")\n",
    "\n",
    "# for idx, doc in enumerate(all_docs):\n",
    "#     print(f\"Document {idx + 1}:\")\n",
    "#     print(doc)\n",
    "#     print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc32f82-5c4e-40eb-a8fe-ebfe58f7a3fc",
   "metadata": {},
   "source": [
    "## Run a similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bedd55-a8c6-4dac-a1d5-c3d4e65b9300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "@chain\n",
    "def retriever(query: str) -> List[Document]:\n",
    "    docs, scores = zip(*vectorstore.similarity_search_with_score(query, k=8))\n",
    "    for doc, score in zip(docs, scores):\n",
    "        doc.metadata[\"score\"] = score\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2196ea26-3310-4f96-86ab-4aaada22b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = \"What are slices?\"\n",
    "embedding = HuggingFaceEmbeddings().embed_query(phrase)\n",
    "\n",
    "results = retriever.invoke(phrase)\n",
    "#print(results)\n",
    "\n",
    "for result in results:\n",
    "    print(result.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yc5e11hmj0b",
   "metadata": {},
   "source": [
    "## Compare Pre-Reranking vs Post-Reranking\n",
    "\n",
    "Retrieves `k` documents via vector similarity, then reranks them with `BAAI/bge-reranker-v2-m3`.\n",
    "Shows rank changes so you can judge whether reranking is meaningfully reordering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vl7cet7zea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "rerank_tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-v2-m3\")\n",
    "rerank_model = AutoModelForSequenceClassification.from_pretrained(\"BAAI/bge-reranker-v2-m3\").to(device)\n",
    "rerank_model.eval()\n",
    "print(\"Reranker loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tccycq3j4zm",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"what is the difference between a slice key and a bastion key?\"  # <<< Change this\n",
    "K = 20                      # documents to retrieve (matches RETRIEVAL_K in .env)\n",
    "\n",
    "# --- Retrieve ---\n",
    "retrieved = vectorstore.similarity_search_with_score(QUERY, k=K)\n",
    "pre_docs  = [doc for doc, _ in retrieved]\n",
    "pre_scores = [score for _, score in retrieved]  # lower = more similar in ChromaDB L2\n",
    "\n",
    "# --- Rerank ---\n",
    "pairs = [(QUERY, doc.page_content) for doc in pre_docs]\n",
    "with torch.no_grad():\n",
    "    inputs = rerank_tokenizer.batch_encode_plus(\n",
    "        pairs, padding=True, truncation=True,\n",
    "        return_tensors=\"pt\", max_length=512\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    rerank_scores = rerank_model(**inputs).logits.squeeze().tolist()\n",
    "\n",
    "if isinstance(rerank_scores, float):\n",
    "    rerank_scores = [rerank_scores]\n",
    "\n",
    "# --- Build comparison table ---\n",
    "pre_rank = {id(doc): i + 1 for i, doc in enumerate(pre_docs)}\n",
    "post_docs = sorted(zip(pre_docs, rerank_scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"Query: '{QUERY}'\")\n",
    "print(f\"{'Post':>4}  {'Pre':>4}  {'Move':>5}  {'Rerank score':>12}  Source\")\n",
    "print(\"-\" * 90)\n",
    "for post_i, (doc, rscore) in enumerate(post_docs, start=1):\n",
    "    pre_i = pre_rank[id(doc)]\n",
    "    move  = pre_i - post_i\n",
    "    arrow = f\"▲{move}\" if move > 0 else (f\"▼{abs(move)}\" if move < 0 else \"  =\")\n",
    "    source = doc.metadata.get(\"source\", doc.page_content[:60].replace(\"\\n\", \" \"))\n",
    "    print(f\"{post_i:>4}  {pre_i:>4}  {arrow:>5}  {rscore:>12.4f}  {source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed84d37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fabric-ai-tool-deployment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
